{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/pkg_resources.py:1031: UserWarning: /home/kudkudak/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from misc.config import *\n",
    "import logging\n",
    "from models.strategy import *\n",
    "\n",
    "from experiments import experiment_runner, fit_active_learning, fit_grid\n",
    "\n",
    "from kaggle_ninja import *\n",
    "\n",
    "from experiments.utils import *\n",
    "from experiment_runner import run_experiment\n",
    "\n",
    "import inspect\n",
    "from get_data import *\n",
    "inspect.getsourcelines(run_experiment)\n",
    "from get_data import _get_raw_data, fingerprints, proteins\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import MDS\n",
    "from get_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from experiments.experiment_runner import run_experiment, run_experiment_grid\n",
    "from experiments import experiment_runner, fit_active_learning, fit_grid\n",
    "from sklearn.svm import SVC\n",
    " \n",
    "grid_result_passive = run_experiment(\"fit_grid\",\n",
    "                                    recalculate_experiments=True,\n",
    "                                    n_jobs = 4, \n",
    "                                    experiment_detailed_name=\"fit_grid_passive\",\n",
    "                                    base_experiment=\"fit_active_learning\",\n",
    "                                    seed=666,\n",
    "                                    grid_params = {\"base_model_kwargs:alpha\": list(np.logspace(-5,5,10))}, \n",
    "                                    base_experiment_kwargs={\"strategy\": \"random_query\",\n",
    "                                                       \"loader_function\": \"get_splitted_uniform_data\",\n",
    "                                                       \"batch_size\": 1, \\\n",
    "                                                       \"base_model\": \"SGDClassifier\",\n",
    "                                                       \"loader_args\": {\"n_folds\": 2}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_monitors([grid_results_uncert.experiments[0], grid_results.experiments[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "loader = [\"get_splitted_data\",\n",
    "          {\"n_folds\": 5,\n",
    "           \"seed\":777,\n",
    "           \"test_size\":0.0}]\n",
    "strategy = \"random_query\"\n",
    "f = \"ExtFP\"\n",
    "grid_results = run_experiment(\"fit_grid\",\n",
    "                       n_jobs = 2, \\\n",
    "                       experiment_detailed_name=\"5ht6_\"+f+\"_\"+strategy,\n",
    "                       base_experiment=\"fit_active_learning\",\n",
    "                       seed=777,\n",
    "                       grid_params = {\"base_model_kwargs:alpha\": list(np.logspace(-5,5,10))},\n",
    "                       base_experiment_kwargs={\"strategy\": strategy,\n",
    "                                               \"loader_function\": loader[0],\n",
    "                                               \"loader_args\": loader[1],\n",
    "                                                \"protein\": \"5ht6\",\n",
    "                                                \"fingerprint\":f,\n",
    "                                               \"batch_size\": 20, \\\n",
    "                                               \"base_model\": \"SGDClassifier\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# preprocess_fncs = [[\"to_binary\", {\"all_below\": True}]]\n",
    "# loader = [\"get_splitted_data_clusterly\", {\n",
    "#         \"seed\": 777, \"preprocess_fncs\": preprocess_fncs, \"n_folds\": 2}]\n",
    "# data = get_data([[\"5ht6\", \"KlekFP\"]], loader, preprocess_fncs, force_reload=True)\n",
    "\n",
    "strategy = \"random_query\"\n",
    "f = \"ExtFP\"\n",
    "grid_results = run_experiment(\"fit_grid\",\n",
    "                       n_jobs = 2, \\\n",
    "                       experiment_detailed_name=\"5ht6_\"+f+\"_\"+strategy,\n",
    "                       base_experiment=\"fit_active_learning\",\n",
    "                       seed=777,\n",
    "                       grid_params = {\"base_model_kwargs:alpha\": list(np.logspace(-5,5,10))},\n",
    "                       base_experiment_kwargs={\"strategy\": strategy,\n",
    "                                               \"loader_function\": loader[0],\n",
    "                                               \"loader_args\": loader[1],\n",
    "                                                \"protein\": \"5ht6\",\n",
    "                                                \"fingerprint\":f,\n",
    "                                               \"batch_size\": 20, \\\n",
    "                                               \"base_model\": \"SGDClassifier\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Has to be agnostic to labels\n",
    "# Can have different clasa distribution - seems a good idea for active learning\n",
    "# Finds K clusters and this defines K folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_combinations = [p for p in list(product(proteins, fingerprints))]\n",
    "\n",
    "preprocess_fncs = [[\"to_binary\", {\"all_below\": True}]]\n",
    "loader = [\"get_splitted_data_clusterly\", {\n",
    "        \"seed\": 777, \"preprocess_fncs\": preprocess_fncs, \"n_folds\": 2}]\n",
    "data = get_data([[\"5ht6\", \"KlekFP\"]], loader, preprocess_fncs, force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from experiments.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_experiment_results(grid_results.experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "all_combinations = [p for p in list(product(proteins, fingerprints))]\n",
    "\n",
    "# comps = all_combinations[len(proteins)+fingerprints.index(\"KlekFP\"):len(proteins)+fingerprints.index(\"KlekFP\")+1]\n",
    "comps = all_combinations[15:16]\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "fig.set_figwidth(10*1)\n",
    "fig.set_figheight(10*1)\n",
    "\n",
    "\n",
    "\n",
    "if hasattr(axes, \"reshape\"):\n",
    "    axes = axes.reshape(-1)\n",
    "else:\n",
    "    axes = [axes]\n",
    "    \n",
    "for ax, comp in zip(axes, comps):\n",
    "    preprocess_fncs = [[\"to_binary\", {\"all_below\": True}]]\n",
    "    m = DBSCAN(eps=0.01, metric='precomputed')\n",
    "    \n",
    "    X, K = calculate_jaccard_distance(force_reload=True, protein=comp[0], fingerprint=comp[1], \\\n",
    "                                               seed=777, preprocess_fncs=preprocess_fncs)\n",
    "    clusters = m.fit_predict(K)\n",
    "    \n",
    "    \n",
    "    _, Y = _get_raw_data(comp[0], comp[1])\n",
    "\n",
    "        \n",
    "    colors = ['rgb'[int(c)] for c in clusters]\n",
    "\n",
    "    X_pca = get_PCA_of_raw_data(n_components=10, iterated_power=10, protein=comp[0], fingerprint=comp[1])\n",
    "    ax.legend()\n",
    "    ax.set_title(\"_\".join(comp))\n",
    "    ax.scatter(X_pca[:, 0], X_pca[:, 1], marker='o', c=colors)\n",
    "    ax.legend()\n",
    "# plt.savefig('foo.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "all_combinations = [p for p in list(product(proteins, fingerprints))]\n",
    "prot_to_draw = 1 # len(proteins)\n",
    "fing_to_draw = len(fingerprints)\n",
    "comps = all_combinations[0:fing_to_draw*prot_to_draw]\n",
    "fig, axes = plt.subplots(prot_to_draw, fing_to_draw)\n",
    "fig.set_figwidth(10*fing_to_draw)\n",
    "fig.set_figheight(10*prot_to_draw)\n",
    "\n",
    "\n",
    "if hasattr(axes, \"reshape\"):\n",
    "    axes = axes.reshape(-1)\n",
    "else:\n",
    "    axes = [axes]\n",
    "    \n",
    "for ax, comp in zip(axes, comps):\n",
    "    X, Y = _get_raw_data(comp[0], comp[1])\n",
    "    X_pca = get_PCA_of_raw_data(n_components=10, iterated_power=10, protein=comp[0], fingerprint=comp[1])\n",
    "    ax.legend()\n",
    "    ax.set_title(\"_\".join(comp))\n",
    "    for c, i, target_name in zip(\"rgb\", [-1, 1], [\"inactive\", \"active\"]):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        ax.scatter(X_pca[Y == i, 0], X_pca[Y == i, 1], c=c, label=target_name)\n",
    "    ax.legend()\n",
    "# plt.savefig('foo.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_folds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "protein = \"5ht6\"\n",
    "fingerprint = \"SubFP\"\n",
    "seed = 777\n",
    "cluster_size_threshold = 0.1\n",
    "preprocess_fncs = [[\"to_binary\", {\"all_below\": True}]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@cached()\n",
    "def get_PCA_of_raw_data(n_components, iterated_power, protein, fingerprint, only_positive=False):\n",
    "    X, Y = _get_raw_data(protein, fingerprint)\n",
    "    if only_positive:\n",
    "        X = X[Y==1]\n",
    "    model = RandomizedPCA(n_components=n_components,iterated_power=iterated_power).fit(X.toarray(), Y)\n",
    "    return model.transform(X.toarray())\n",
    "\n",
    "@cached()\n",
    "def calculate_jaccard_distance(protein, fingerprint, seed, preprocess_fncs, only_positive=False):\n",
    "    loader = [\"get_splitted_data\",\n",
    "              {\"n_folds\": 1,\n",
    "               \"seed\":seed,\n",
    "               \"test_size\":0.0}]\n",
    "    data = get_data([[protein, fingerprint]], loader, preprocess_fncs)\n",
    "    Y = data[protein+\"_\"+fingerprint][0][0][\"Y_train\"]\n",
    "    X1T = data[protein+\"_\"+fingerprint][0][0][\"X_train\"]\n",
    "    if only_positive:\n",
    "        X1T = X1T[Y==1]\n",
    "    X2T = X1T\n",
    "    X1T_sums = np.array(X1T.sum(axis=1))\n",
    "    X2T_sums = np.array(X2T.sum(axis=1))\n",
    "    K = X1T.dot(X2T.T)\n",
    "    K = K.toarray()\n",
    "    K2 = -(K.copy())\n",
    "    K2 += (X1T_sums.reshape(-1,1))\n",
    "    K2 += (X2T_sums.reshape(1,-1))\n",
    "    K = K/K2\n",
    "    return X1T, 1 - K\n",
    "\n",
    "@cached()\n",
    "def get_MDS_of_raw_data(n_components,  protein, fingerprint, seed, preprocess_fncs, only_positive=False):\n",
    "    X, K = calculate_jaccard_distance(protein=protein, fingerprint=fingerprint, \\\n",
    "                                      seed=seed, preprocess_fncs=preprocess_fncs, only_positive=only_positive)\n",
    "    print K.shape\n",
    "    print X.shape\n",
    "    m = MDS(n_components=n_components, n_jobs=4, dissimilarity=\"precomputed\")\n",
    "    return m, m.fit_transform(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "protein, fingerprint = proteins[1], fingerprints[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from get_data import fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pca = get_PCA_of_raw_data(force_reload=True, \\\n",
    "                                 n_components=10, iterated_power=10, protein=protein, fingerprint=fingerprint, \n",
    "                            only_positive=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(c[\"DATA_DIR\"], \"actives\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(pd.DataFrame.from_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from misc.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_jaccard_kernel(X1T, X2T):\n",
    "    X1T_sums = np.array(X1T.sum(axis=1))\n",
    "    X2T_sums = np.array(X2T.sum(axis=1))\n",
    "    K = X1T.dot(X2T.T)\n",
    "    if hasattr(K, \"toarray\"):\n",
    "        K = K.toarray()\n",
    "    K2 = -(K.copy())\n",
    "    K2 += (X1T_sums.reshape(-1,1))\n",
    "    K2 += (X2T_sums.reshape(1,-1))\n",
    "    K = K.astype(\"float32\")/K2\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_folds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "protein = \"5ht2a\"\n",
    "fingerprint = \"ExtFP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_jaccard_kernel(X1T, X2T):\n",
    "    X1T_sums = np.array(X1T.sum(axis=1))\n",
    "    X2T_sums = np.array(X2T.sum(axis=1))\n",
    "    K = X1T.dot(X2T.T)\n",
    "    if hasattr(K, \"toarray\"):\n",
    "        K = K.toarray()\n",
    "    K2 = -(K.copy())\n",
    "    K2 += (X1T_sums.reshape(-1,1))\n",
    "    K2 += (X2T_sums.reshape(1,-1))\n",
    "    K = K.astype(\"float32\")/K2\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "actives, inactives = get_protein_fingerprint_cluster_files(protein, fingerprint)\n",
    "clusters_active = []\n",
    "for a, _ in actives:\n",
    "    clusters_active.append(pd.io.parsers.read_csv(os.path.join(c[\"DATA_DIR\"], a), header=None).as_matrix())\n",
    "\n",
    "inactive = pd.io.parsers.read_csv(os.path.join(c[\"DATA_DIR\"], inactives), header=None).as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairwise_distances(clusters_active[0], X[:, 0:1023], metric='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.max(pairwise_distances(clusters_active[0], X[:, 0:1023], metric='l1'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proteins_sabina = [\"5ht1a\", \"5ht2a\", \"5ht6\", \"5ht7\", \\\n",
    "                   \"alpha1a\", \"beta1\", \"beta2\", \"d1\", \"d2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_active[0] is clusters_active[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "protein = \"beta2\"\n",
    "fingerprint = \"KlekFP\"\n",
    "\n",
    "# Read data, not important\n",
    "actives, inactives = get_protein_fingerprint_cluster_files(protein, fingerprint)\n",
    "\n",
    "print actives, inactives\n",
    "\n",
    "clusters_active = []\n",
    "for a, _ in actives:\n",
    "    clusters_active.append(pd.io.parsers.read_csv(os.path.join(c[\"DATA_DIR\"], a), header=None).as_matrix().astype(\"float32\"))\n",
    "inactive = pd.io.parsers.read_csv(os.path.join(c[\"DATA_DIR\"], inactives), header=None).as_matrix().astype(\"float32\")\n",
    "\n",
    "print \"Sizes: \", [cl.shape[0] for cl in clusters_active]\n",
    "\n",
    "# Report when found shared example\n",
    "for A_id in range(len(clusters_active)):\n",
    "    for B_id in range(len(clusters_active)):\n",
    "        if A_id != B_id and A_id > B_id:\n",
    "            print \"Clusters \", A_id+1, \" and \", B_id+1\n",
    "            # Distances between cluster A and B using L1\n",
    "            D = pairwise_distances(clusters_active[A_id], clusters_active[B_id], metric='l1')\n",
    "            print \"Shared examples # : \", (D==0).sum()\n",
    "            print \"Indices of examples: \"\n",
    "            for ex_id, val in enumerate(D):\n",
    "                if np.min(val) == 0:\n",
    "                    print \"(\", ex_id, np.argmin(val), \")\"\n",
    "                    assert all(clusters_active[A_id][ex_id] == clusters_active[B_id][np.argmin(val)])\n",
    "            print \"A.shape[0]=\", clusters_active[A_id].shape[0], \"B.shape[0]=\", clusters_active[B_id].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "def get_protein_fingerprint_cluster_files(protein, fingerprint):\n",
    "    inactives = [\"inactives/\"+x for x in os.listdir(os.path.join(c[\"DATA_DIR\"], \"inactives\")) if x.startswith(protein + \"_inactives\") and x.endswith(fingerprint+\".csv\")]\n",
    "    actives = [(\"actives/\"+x, int(x.split(\"_\")[2][-1])) for x in os.listdir(os.path.join(c[\"DATA_DIR\"], \"actives\")) if x.startswith(protein + \"_actives\") and x.endswith(fingerprint + \".csv\")]\n",
    "    # This magic lines ensures there are no clusters of id > 9\n",
    "    assert(sorted(list(set([a[1] for a in actives]))) == sorted([a[1] for a in actives]))\n",
    "    return sorted(actives, key=lambda x: x[1]), inactives[0]\n",
    "\n",
    "def get_sabina_clusters(protein, fingerprint):\n",
    "\n",
    "    # Read data\n",
    "    actives, inactives = get_protein_fingerprint_cluster_files(protein, fingerprint)\n",
    "    clusters_active = []\n",
    "    for a, _ in actives:\n",
    "        clusters_active.append(pd.io.parsers.read_csv(os.path.join(c[\"DATA_DIR\"], a), header=None).as_matrix().astype(\"float32\"))\n",
    "\n",
    "    inactive = pd.io.parsers.read_csv(os.path.join(c[\"DATA_DIR\"], inactives), header=None).as_matrix().astype(\"float32\")\n",
    "\n",
    "    # Standarize sizes\n",
    "    max_cols = max(inactive.shape[1], max(cl.shape[1] for cl in clusters_active))\n",
    "    for cl_id, cl in enumerate(clusters_active):\n",
    "        if cl.shape[1] != max_cols:\n",
    "            clusters_active[cl_id] = np.hstack([cl, np.zeros(shape=(cl.shape[0], max_cols - cl.shape[1]))])\n",
    "    if inactive.shape[1] != max_cols:\n",
    "            inactive = np.hstack([inactive, np.zeros(shape=(inactive.shape[0], max_cols - inactive.shape[1]))])\n",
    "\n",
    "\n",
    "    # Start with biggest\n",
    "    biggest_id = np.argsort([-cluster.shape[0] for cluster in clusters_active])[0]\n",
    "    X = clusters_active[biggest_id]\n",
    "\n",
    "    clusters_active_ids = [range(X.shape[0])]\n",
    "    max_id = X.shape[0] - 1\n",
    "    for cluster_id, cluster in enumerate(clusters_active):\n",
    "        if cluster_id != biggest_id:\n",
    "            if not np.isfinite(cluster).all():\n",
    "                raise ValueError(\"F*CK, nan in cluster file.\")\n",
    "            similarities = np.min(pairwise_distances(cluster, X, metric='l1'), axis=1)\n",
    "            X = np.vstack([X, cluster[similarities!=0]])\n",
    "            start_id = max_id + 1\n",
    "            existing = list(np.where(similarities==0)[0])\n",
    "            clusters_active_ids.append(existing + range(start_id, start_id +  (similarities!=0).sum()))\n",
    "            max_id = max(max_id, max(clusters_active_ids[-1])) # Update max_id\n",
    "            assert(len(clusters_active_ids[-1]) == cluster.shape[0])\n",
    "      \n",
    "    # Add inactives\n",
    "    X = np.vstack([X, inactive ])\n",
    "    inactive_ids = range(max_id+1, max_id+1+inactive.shape[0])\n",
    "    \n",
    "    # Labels\n",
    "    Y = np.zeros(shape=(X.shape[0], 1))\n",
    "    Y[:] = 1\n",
    "    Y[max_id+1:] = -1\n",
    "    \n",
    "    return X, Y, [np.array(cl).reshape(-1) for cl in clusters_active_ids], np.array(inactive_ids).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y, clusters_active_ids, inactive_ids = get_sabina_clusters(\"5ht1a\", \"SubFP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def can_be_A_easier(i, X, clusters_active_ids, inactive):\n",
    "    # A cluster is a good A-cluster candidate if it is big and the rest of active clusters are big\n",
    "    difference_A_C = [ len(set(cl_ids).difference(clusters_active_ids[i])) for cl_ids in clusters_active_ids]\n",
    "    \n",
    "    active_size = X.shape[0] - inactive.shape[0]\n",
    "    print difference_A_C, sum(difference_A_C), active_size\n",
    "    return \\\n",
    "        sum(difference_A_C) > 0.1*active_size \\\n",
    "        and len(clusters_active_ids[i]) > 0.4*active_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def can_be_A(i):\n",
    "    # A cluster is a good A-cluster candidate if there exists another cluster that has at least 10% of\n",
    "    # disjoint members and is itself > 50%\n",
    "    difference_A_C = [ len(set(cl_ids).difference(clusters_active_ids[i])) for cl_ids in clusters_active_ids]\n",
    "    difference_C_A = [ len(set(clusters_active_ids[i]).difference(cl_ids)) for cl_ids in clusters_active_ids]\n",
    "    # Idea is that at least one pair (A,C) of clusters has at least 10% unique members in A\\C and C\\A\n",
    "    print len(clusters_active_ids[i])\n",
    "    print difference_A_C\n",
    "    print difference_C_A\n",
    "    print 0.1 * X.shape[0]\n",
    "    print [size_ac > 0.1 * X.shape[0] and size_ca > 0.06 * X.shape[0] for size_ac, size_ca in zip(difference_A_C, difference_C_A)]\n",
    "    return \\\n",
    "        any(size_ac > 0.1 * X.shape[0] and size_ca > 0.06 * X.shape[0] \n",
    "                for size_ac, size_ca in zip(difference_A_C, difference_C_A)) \\\n",
    "        and len(clusters_active_ids[i]) > 0.5*X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pick A cluster\n",
    "s = []\n",
    "sizes = []\n",
    "for p in proteins_sabina:\n",
    "    try:\n",
    "        X, Y, clusters_active_ids, inactive_ids = get_sabina_clusters(p, \"KlekFP\")\n",
    "#         print([cl.shape[0] for cl in clusters_active_ids])\n",
    "        sizes.append([cl.shape[0] for cl in clusters_active_ids])\n",
    "        A_candidates = [(cl.shape[0], id) for id, cl in enumerate(clusters_active_ids) if can_be_A_easier(id, X, clusters_active_ids, inactive_ids)]\n",
    "\n",
    "        if len(A_candidates) == 0:\n",
    "            print \"Not satisfied for \", p\n",
    "            s.append(\"Not satisfied\")\n",
    "        else:\n",
    "            print \"OK\"\n",
    "            s.append(\"OK\")\n",
    "    except Exception, e:\n",
    "        print e\n",
    "        sizes.append([])\n",
    "        s.append(str(e))\n",
    "#         raise ValueError(\"No cluster qualified for experiment A-type cluster. F*CK\")\n",
    "#     np.max(A_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p, result, size in zip(proteins_sabina, s, sizes):\n",
    "    print \"Protein: \",p\n",
    "    print \"Satisfied?\", result\n",
    "    print \"Sizes of clusters\", size\n",
    "    print \"-----\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Keep track of indices\n",
    "size = sum(c.shape[0] for c in clusters_active) + inactive.shape[0]\n",
    "cluster_id = np.zeros(shape=(size,1))\n",
    "start_id = 0\n",
    "for c_id, cluster in enumerate(clusters_active):\n",
    "    cluster_id[start_id:start_id +cluster.shape[0]] = c_id\n",
    "    start_id += cluster.shape[0]\n",
    "cluster_id[start_id:] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assert that we have sensible indexing\n",
    "assert(all(list(cluster_id[clusters_active[0].shape[0]: clusters_active[0].shape[0] + clusters_active[1].shape[0]] == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_active[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we pick biggest one denoted as A\n",
    "A_id = np.argsort([-cluster.shape[0] for cluster in clusters_active])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarities = calculate_jaccard_kernel(clusters_active[0], clusters_active[A_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if random picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models.strategy import jaccard_similarity_score_fast\n",
    "\n",
    "# Some magic: assigning each inactive to the closest active cluster by mean\n",
    "similarities = []\n",
    "for cluster in clusters_active:\n",
    "    similarities.append(calculate_jaccard_kernel(inactive, cluster).mean(axis=1).reshape(-1,1))\n",
    "# Each col has mean distance of col-th cluster to row-th inactive example\n",
    "similarities = np.hstack(similarities) \n",
    "\n",
    "# Do assignment\n",
    "active_size = sum(cluster.shape[0] for cluster in clusters_active)\n",
    "for i in range(inactive.shape[0]):\n",
    "    cluster_id[i + active_size] = np.argmax(similarities[i,:])\n",
    "    \n",
    "# Assert that similarities is what we want\n",
    "assert np.abs(similarities[10,1] - np.mean([jaccard_similarity_score_fast(inactive[10], clusters_active[1][i]) for i in range(clusters_active[1].shape[0])])) < 1e-3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[cluster.shape[0] for cluster in clusters_active]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = calculate_jaccard_kernel(clusters_active[0], clusters_active[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_active[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if n_folds == 1:\n",
    "    fold_indices = [[range(y.shape[0]), None]]\n",
    "else:\n",
    "    fold_indices = StratifiedKFold(cluster_id, n_folds=n_folds, shuffle=True, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_active[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_active[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(15,15))\n",
    "# f, ax = plt.subplots(1,1)\n",
    "# for j, cluster in enumerate(clusters[0:2]):\n",
    "#     X_pca_picked = X_pca[colors==j,:]\n",
    "#     cols = np.array([j for k in range(X_pca_picked.shape[0])])\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], s=40, alpha=0.9, cmap=plt.get_cmap(\"Pastel1\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interestingness_index(X, big_subtrees[0], big_subtrees[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(big_subtrees[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interestingness_index(X, big_subtrees[0], big_subtrees[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(big_subtrees[0]).intersection(set(big_subtrees[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(len(subtree) > check_threshold for subtree in id_to_nodes.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphviz as gv\n",
    "g = gv.Graph(format='svg')\n",
    "for i in range(K.shape[0]):\n",
    "    g.node(str(i))\n",
    "# Nodes in m.children_ are sorted by merging time\n",
    "for id, n in enumerate(m.children_):\n",
    "    g.node(str(id + K.shape[0]))\n",
    "    g.edge(str(id + K.shape[0]), str(n[0]))\n",
    "    g.edge(str(id + K.shape[0]), str(n[1]))\n",
    "    id_to_nodes[id  + K.shape[0]] = id_to_nodes[n[0]] + id_to_nodes[n[1]]\n",
    "g.render(\"clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(clusters==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@cached()\n",
    "def get_PCA_of_raw_data(n_components, iterated_power, protein, fingerprint, only_positive=False):\n",
    "    X, Y = _get_raw_data(protein, fingerprint)\n",
    "    if only_positive:\n",
    "        X = X[Y==1]\n",
    "    model = RandomizedPCA(n_components=n_components,iterated_power=iterated_power).fit(X.toarray(), Y)\n",
    "    return model.transform(X.toarray())\n",
    "\n",
    "\n",
    "\n",
    "@cached()\n",
    "def get_MDS_of_raw_data(n_components,  protein, fingerprint, seed, preprocess_fncs, only_positive=False):\n",
    "    X, K = calculate_jaccard_distance(protein=protein, fingerprint=fingerprint, \\\n",
    "                                      seed=seed, preprocess_fncs=preprocess_fncs, only_positive=only_positive)\n",
    "    print K.shape\n",
    "    print X.shape\n",
    "    m = MDS(n_components=n_components, n_jobs=4, dissimilarity=\"precomputed\")\n",
    "    return m, m.fit_transform(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "all_combinations = [p for p in list(product(proteins, fingerprints))]\n",
    "prot_to_draw = 1 #len(proteins)\n",
    "fing_to_draw = len(fingerprints)\n",
    "comps = all_combinations[len(fingerprints): len(fingerprints)*2]\n",
    "fig, axes = plt.subplots(prot_to_draw, fing_to_draw)\n",
    "fig.set_figwidth(20*fing_to_draw)\n",
    "fig.set_figheight(20*prot_to_draw)\n",
    "preprocess_fncs = [['to_binary', {'all_below': True}]]\n",
    "\n",
    "\n",
    "if hasattr(axes, \"reshape\"):\n",
    "    axes = axes.reshape(-1)\n",
    "else:\n",
    "    axes = [axes]\n",
    "    \n",
    "for ax, comp in zip(axes, comps):\n",
    "    print comp\n",
    "#     X, Y = _get_raw_data(comp[0], comp[1])\n",
    "    X_mds = get_MDS_of_raw_data(n_components=2, seed=777, protein=comp[0], \\\n",
    "                                fingerprint=comp[1], preprocess_fncs=preprocess_fncs, only_positive=True)\n",
    "#     X_mds = get_PCA_of_raw_data(force_reload=True, \\\n",
    "#                                 n_components=10, iterated_power=10, protein=comp[0], fingerprint=comp[1])  \n",
    "    ax.legend()\n",
    "    ax.set_title(\"_\".join(comp))\n",
    "    ax.scatter(X_mds[:, 0], X_mds[:, 1])\n",
    "    ax.legend()\n",
    "plt.savefig('pca_5ht6_one_class_2_comp.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "all_combinations = [p for p in list(product(proteins, fingerprints))]\n",
    "prot_to_draw = 1 #len(proteins)\n",
    "fing_to_draw = len(fingerprints)\n",
    "comps = all_combinations[0:fing_to_draw*prot_to_draw]\n",
    "fig, axes = plt.subplots(prot_to_draw, fing_to_draw)\n",
    "fig.set_figwidth(10*fing_to_draw)\n",
    "fig.set_figheight(10*prot_to_draw)\n",
    "\n",
    "if hasattr(axes, \"reshape\"):\n",
    "    axes = axes.reshape(-1)\n",
    "else:\n",
    "    axes = [axes]\n",
    "    \n",
    "for ax, comp in zip(axes, comps):\n",
    "    X, Y = _get_raw_data(comp[0], comp[1])\n",
    "    X = X[Y==1] # Pick only positive\n",
    "    X_pca = get_PCA_of_raw_data(force_reload=True, \\\n",
    "                                n_components=10, iterated_power=10, protein=comp[0], fingerprint=comp[1])\n",
    "    ax.legend()\n",
    "    ax.set_title(\"_\".join(comp))\n",
    "    ax.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "    ax.legend()\n",
    "plt.savefig('foo.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_best(grid_results_random.experiments, metric=\"mean_mcc_valid\").results['mean_mcc_valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_best(grid_results_uncert.experiments, metric=\"mean_mcc_valid\").results['mean_mcc_valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
